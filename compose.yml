# to run the compose file: docker compose up -d --force-recreate --build
name: source_chat
services:

  # after container is started,
  # run 'ollama pull mxbai-embed-large:335m' command manually in container's terminal
  # or run 'docker exec -it ollama ollama pull mxbai-embed-large:335m' command from a terminal(outside of container)
  # please refer to the https://ollama.com/blog/embedding-models to find a list of embedding models
  # you can use the following request to test
  # curl http://localhost:11434/api/embed -d '{
  #   "model": "mxbai-embed-large",
  #   "input": "Llamas are members of the camelid family"
  # }'
  ollama:
    # https://hub.docker.com/r/ollama/ollama
    image: ollama/ollama:latest
    container_name: ollama
    hostname: ollama
    ports:
      - "127.0.0.1:11434:11434"
    pull_policy: always
    restart: unless-stopped
    tty: true
    volumes:
      - ./container_volumes/ollama:/root/.ollama
